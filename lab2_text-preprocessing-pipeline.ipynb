{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2: Text Preprocessing Pipeline\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**Objectives:**\n",
    "- Understand and implement tokenization techniques\n",
    "- Apply stemming and lemmatization for text normalization\n",
    "- Remove stop words and special characters\n",
    "- Build a complete text preprocessing pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Complete all the exercises marked with `# TODO`\n",
    "2. Run each cell to verify your answers\n",
    "3. Save your completed notebook\n",
    "4. **Push your work to a Git repository and send the link to: yoroba93@gmail.com**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rishita\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: click in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rishita\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.2 MB 4.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.6/14.2 MB 3.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.8/14.2 MB 3.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.4/14.2 MB 3.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.9/14.2 MB 2.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.4/14.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.7/14.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.2/14.2 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 4.5/14.2 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.2/14.2 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.3/14.2 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.8/14.2 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 7.3/14.2 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.9/14.2 MB 2.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.4/14.2 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.9/14.2 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.4/14.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.0/14.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.5/14.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.0/14.2 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.5/14.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.3/14.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/14.2 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.6/14.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl (118 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.8 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 524.3/654.8 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 654.8/654.8 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.3.3-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 3.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.2 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.2/6.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: wasabi, spacy-loggers, spacy-legacy, murmurhash, cymem, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 wasabi-1.1.3 weasel-0.4.3\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.6/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.9/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.5/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.8/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "!pip install nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Rishita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Download and load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens), typically words or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Tokenization with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic split(): ['Natural', 'Language', 'Processing', 'is', 'fascinating!']\n",
      "Last token: fascinating!\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenization using split()\n",
    "text = \"Natural Language Processing is fascinating!\"\n",
    "\n",
    "# Basic split on whitespace\n",
    "tokens_basic = text.split()\n",
    "print(\"Basic split():\", tokens_basic)\n",
    "\n",
    "# Problem: punctuation is attached to words!\n",
    "print(\"Last token:\", tokens_basic[-1])  # 'fascinating!' includes the !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens: ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'I', \"'m\", 'learning', 'NLP', '.', 'It', \"'s\", 'really', 'interesting', '.']\n",
      "\n",
      "Sentence tokens:\n",
      "  1. Hello!\n",
      "  2. How are you doing today?\n",
      "  3. I'm learning NLP.\n",
      "  4. It's really interesting.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello! How are you doing today? I'm learning NLP. It's really interesting.\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word tokens:\", word_tokens)\n",
    "\n",
    "# Sentence tokenization\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(\"\\nSentence tokens:\")\n",
    "for i, sent in enumerate(sent_tokens):\n",
    "    print(f\"  {i+1}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic split: ['I', \"can't\", 'believe', \"it's\", 'not', 'butter!', \"Don't\", 'you', 'think', 'so?']\n",
      "NLTK tokenize: ['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'not', 'butter', '!', 'Do', \"n't\", 'you', 'think', 'so', '?']\n"
     ]
    }
   ],
   "source": [
    "# NLTK handles contractions and punctuation better\n",
    "text = \"I can't believe it's not butter! Don't you think so?\"\n",
    "\n",
    "print(\"Basic split:\", text.split())\n",
    "print(\"NLTK tokenize:\", word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Dr.', 'Smith', \"'s\", 'patients', 'ca', \"n't\", 'understand', 'why', 'they', \"'re\", 'feeling', 'unwell', '.', 'Is', 'it', 'the', 'flu', '?']\n",
      "Number of tokens: 18\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 1.1\n",
    "# Tokenize the following text into words using NLTK\n",
    "# Count how many tokens are produced\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Uncomment the next line ONLY if punkt is not already downloaded\n",
    "# nltk.download('punkt')\n",
    "\n",
    "text = \"Dr. Smith's patients can't understand why they're feeling unwell. Is it the flu?\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Number of tokens:\", num_tokens)\n",
    "\n",
    "assert num_tokens == 18, f\"Expected 18 tokens, got {num_tokens}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.']\n",
      "\n",
      "Detailed token info:\n",
      "  Apple        | POS: PROPN  | Is Stop: False\n",
      "  is           | POS: AUX    | Is Stop: True\n",
      "  looking      | POS: VERB   | Is Stop: False\n",
      "  at           | POS: ADP    | Is Stop: True\n",
      "  buying       | POS: VERB   | Is Stop: False\n",
      "  U.K.         | POS: PROPN  | Is Stop: False\n",
      "  startup      | POS: VERB   | Is Stop: False\n",
      "  for          | POS: ADP    | Is Stop: True\n",
      "  $            | POS: SYM    | Is Stop: False\n",
      "  1            | POS: NUM    | Is Stop: False\n",
      "  billion      | POS: NUM    | Is Stop: False\n",
      "  .            | POS: PUNCT  | Is Stop: False\n"
     ]
    }
   ],
   "source": [
    "# spaCy tokenization\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# spaCy provides additional information\n",
    "print(\"\\nDetailed token info:\")\n",
    "for token in doc:\n",
    "    print(f\"  {token.text:12} | POS: {token.pos_:6} | Is Stop: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'Is', \"n't\", 'it', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 1.2\n",
    "# Use spaCy to tokenize the text and extract only:\n",
    "# 1. Tokens that are NOT punctuation\n",
    "# 2. Tokens that are NOT spaces\n",
    "\n",
    "text = \"The quick, brown fox jumps over the lazy dog! Isn't it amazing?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "clean_tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "print(\"Clean tokens:\", clean_tokens)\n",
    "assert len(clean_tokens) == 13, f\"Expected 13 tokens, got {len(clean_tokens)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Different Tokenization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regexp tokens: ['Hello', 'How', 's', 'it', 'going', 'NLP', 'user123']\n",
      "Tweet tokens: ['Hello', '!', \"How's\", 'it', 'going', '?', '#NLP', '@user123']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "\n",
    "# RegexpTokenizer - tokenize using a custom pattern\n",
    "# \\w+ matches word characters only (removes punctuation)\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "text = \"Hello! How's it going? #NLP @user123\"\n",
    "print(\"Regexp tokens:\", regexp_tokenizer.tokenize(text))\n",
    "\n",
    "# TweetTokenizer - designed for social media text\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "print(\"Tweet tokens:\", tweet_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabetic tokens: ['I', 'have', 'cats', 'and', 'dogs', 'Their', 'names', 'are', 'Max', 'and', 'Bella']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 1.3\n",
    "# Create a RegexpTokenizer that extracts only alphabetic words (no numbers, no punctuation)\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"I have 3 cats and 2 dogs! Their names are Max123 and Bella.\"\n",
    "\n",
    "# Tokenizer that keeps only alphabetic sequences\n",
    "alpha_tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "alpha_tokens = alpha_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Alphabetic tokens:\", alpha_tokens)\n",
    "assert alpha_tokens == ['I', 'have', 'cats', 'and', 'dogs', 'Their', 'names', 'are', 'Max', 'and', 'Bella'], \"Check your pattern!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Stemming and Lemmatization\n",
    "\n",
    "Both techniques reduce words to their base form, but they work differently:\n",
    "- **Stemming**: Chops off word endings using rules (faster, cruder)\n",
    "- **Lemmatization**: Uses vocabulary and morphological analysis (slower, accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Stemming with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "  running      -> run\n",
      "  runs         -> run\n",
      "  ran          -> ran\n",
      "  runner       -> runner\n",
      "  easily       -> easili\n",
      "  fairly       -> fairli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "# Porter Stemmer (most common)\n",
    "porter = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n",
    "\n",
    "print(\"Porter Stemmer:\")\n",
    "for word in words:\n",
    "    print(f\"  {word:12} -> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball Stemmer:\n",
      "  running      -> run\n",
      "  runs         -> run\n",
      "  ran          -> ran\n",
      "  runner       -> runner\n",
      "  easily       -> easili\n",
      "  fairly       -> fair\n",
      "\n",
      "Available languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer (supports multiple languages)\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print(\"Snowball Stemmer:\")\n",
    "for word in words:\n",
    "    print(f\"  {word:12} -> {snowball.stem(word)}\")\n",
    "\n",
    "# Available languages\n",
    "print(\"\\nAvailable languages:\", SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming can produce non-words:\n",
      "  studies      -> studi\n",
      "  studying     -> studi\n",
      "  university   -> univers\n",
      "  universe     -> univers\n",
      "  beautiful    -> beauti\n",
      "  beauty       -> beauti\n"
     ]
    }
   ],
   "source": [
    "# Stemming limitations - can produce non-words\n",
    "problem_words = [\"studies\", \"studying\", \"university\", \"universe\", \"beautiful\", \"beauty\"]\n",
    "\n",
    "print(\"Stemming can produce non-words:\")\n",
    "for word in problem_words:\n",
    "    print(f\"  {word:12} -> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['the', 'cats', 'are', 'running', 'and', 'jumping', 'over', 'the', 'sleeping', 'dogs']\n",
      "Stemmed: ['the', 'cat', 'are', 'run', 'and', 'jump', 'over', 'the', 'sleep', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 2.1\n",
    "# Apply Porter stemming to all words in the sentence\n",
    "# Return the stemmed tokens as a list\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Uncomment if needed\n",
    "# nltk.download('punkt')\n",
    "\n",
    "sentence = \"The cats are running and jumping over the sleeping dogs\"\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "# Step 2: Apply stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Stemmed:\", stemmed_tokens)\n",
    "\n",
    "assert stemmed_tokens == ['the', 'cat', 'are', 'run', 'and', 'jump', 'over', 'the', 'sleep', 'dog'], \"Check your stemming!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization (default - assumes nouns):\n",
      "  running      -> running\n",
      "  runs         -> run\n",
      "  ran          -> ran\n",
      "  better       -> better\n",
      "  studies      -> study\n",
      "  feet         -> foot\n",
      "  geese        -> goose\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"better\", \"studies\", \"feet\", \"geese\"]\n",
    "\n",
    "print(\"Lemmatization (default - assumes nouns):\")\n",
    "for word in words:\n",
    "    print(f\"  {word:12} -> {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with POS tags:\n",
      "  running (verb):     run\n",
      "  running (noun):     running\n",
      "  better (adjective): good\n",
      "  studies (verb):     study\n",
      "  studies (noun):     study\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization works better with POS tags\n",
    "# pos: 'n' = noun, 'v' = verb, 'a' = adjective, 'r' = adverb\n",
    "\n",
    "print(\"Lemmatization with POS tags:\")\n",
    "print(f\"  running (verb):     {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"  running (noun):     {lemmatizer.lemmatize('running', pos='n')}\")\n",
    "print(f\"  better (adjective): {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "print(f\"  studies (verb):     {lemmatizer.lemmatize('studies', pos='v')}\")\n",
    "print(f\"  studies (noun):     {lemmatizer.lemmatize('studies', pos='n')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization results:\n",
      "  flying       (v) -> fly\n",
      "  happily      (r) -> happily\n",
      "  worse        (a) -> bad\n",
      "  mice         (n) -> mouse\n",
      "  are          (v) -> be\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 2.2 \n",
    "# Lemmatize the following words using the correct POS tag \n",
    "# Fill in the POS tag for each word\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Format: (word, pos_tag)\n",
    "words_with_pos = [\n",
    "    (\"flying\", \"v\"),      # verb -> fly\n",
    "    (\"happily\", \"r\"),     # adverb -> happily\n",
    "    (\"worse\", \"a\"),       # adjective -> bad\n",
    "    (\"mice\", \"n\"),        # noun -> mouse\n",
    "    (\"are\", \"v\"),         # verb -> be\n",
    "]\n",
    "\n",
    "print(\"Lemmatization results:\")\n",
    "for word, pos in words_with_pos:\n",
    "    lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "    print(f\"  {word:12} ({pos}) -> {lemma}\")\n",
    "\n",
    "# Verify your answers\n",
    "expected = ['fly', 'happily', 'bad', 'mouse', 'be']\n",
    "results = [lemmatizer.lemmatize(w, pos=p) for w, p in words_with_pos]\n",
    "assert results == expected, f\"Expected {expected}, got {results}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy lemmatization (automatic POS detection):\n",
      "  The          (DET  ) -> the\n",
      "  children     (NOUN ) -> child\n",
      "  are          (AUX  ) -> be\n",
      "  playing      (VERB ) -> play\n",
      "  toys         (NOUN ) -> toy\n",
      "  They         (PRON ) -> they\n",
      "  were         (AUX  ) -> be\n",
      "  running      (VERB ) -> run\n",
      "  jumping      (VERB ) -> jump\n"
     ]
    }
   ],
   "source": [
    "# spaCy automatically determines POS and lemmatizes correctly\n",
    "text = \"The children are playing with their toys. They were running and jumping happily.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"spaCy lemmatization (automatic POS detection):\")\n",
    "for token in doc:\n",
    "    if token.text != token.lemma_:  # Only show words that change\n",
    "        print(f\"  {token.text:12} ({token.pos_:5}) -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas: ['the', 'dog', 'be', 'bark', 'loudly', 'at', 'the', 'cat', 'who', 'be', 'climb', 'the', 'tree']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 2.3\n",
    "# Use spaCy to extract the lemmas of all non-punctuation tokens\n",
    "# Return as a list of lowercase lemmas\n",
    "\n",
    "text = \"The dogs were barking loudly at the cats who were climbing the trees.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "lemmas = [token.lemma_.lower() for token in doc if not token.is_punct]\n",
    "\n",
    "print(\"Lemmas:\", lemmas)\n",
    "assert lemmas == ['the', 'dog', 'be', 'bark', 'loudly', 'at', 'the', 'cat', 'who', 'be', 'climb', 'the', 'tree'], \"Check your lemmatization!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Stemming vs Lemmatization Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word         Stemmed      Lemmatized  \n",
      "------------------------------------\n",
      "studies      studi        study       \n",
      "studying     studi        studying    \n",
      "better       better       better      \n",
      "feet         feet         foot        \n",
      "ran          ran          ran         \n",
      "easily       easili       easily      \n",
      "fairly       fairli       fairly      \n",
      "wolves       wolv         wolf        \n"
     ]
    }
   ],
   "source": [
    "# Compare the two approaches\n",
    "words = [\"studies\", \"studying\", \"better\", \"feet\", \"ran\", \"easily\", \"fairly\", \"wolves\"]\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(f\"{'Word':<12} {'Stemmed':<12} {'Lemmatized':<12}\")\n",
    "print(\"-\" * 36)\n",
    "for word in words:\n",
    "    stemmed = porter.stem(word)\n",
    "    # For comparison, we'll use noun as default\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:<12} {stemmed:<12} {lemmatized:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Differences:**\n",
    "- Stemming is faster but may produce non-words (\"studi\", \"easili\")\n",
    "- Lemmatization produces valid words but is slower\n",
    "- Lemmatization requires POS information for best results\n",
    "- Use stemming for speed (search engines), lemmatization for accuracy (chatbots, NLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Stop Words and Special Characters\n",
    "\n",
    "Stop words are common words that usually don't carry much meaning (the, is, at, which, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stop Words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NLTK stop words: 198\n",
      "\n",
      "Sample stop words: [\"that'll\", \"shouldn't\", 'at', 'out', 'couldn', 'them', 'your', 'be', 'hadn', 'than', \"don't\", \"i'd\", \"she's\", 'o', 'too', 'while', 'through', \"isn't\", 'once', 're']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stop words\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Number of NLTK stop words: {len(stop_words_nltk)}\")\n",
    "print(f\"\\nSample stop words: {list(stop_words_nltk)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'in', 'the', 'park']\n",
      "Without stop words: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'park']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from text\n",
    "text = \"The quick brown fox jumps over the lazy dog in the park\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words_nltk]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Without stop words:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tokens: ['sample', 'sentence', 'showing', 'removal', 'stop', 'words', 'text']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 3.1\n",
    "# Remove stop words from the following text and return the remaining tokens\n",
    "# Make sure to lowercase the text first!\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Uncomment if needed\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "text = \"This is a sample sentence showing the removal of stop words from the text\"\n",
    "\n",
    "# Step 1: Lowercase and tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Step 2: Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "print(\"Filtered tokens:\", filtered)\n",
    "assert filtered == ['sample', 'sentence', 'showing', 'removal', 'stop', 'words', 'text'], \"Check your filtering!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Stop Words with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token analysis:\n",
      "  This            is_stop: True\n",
      "  is              is_stop: True\n",
      "  a               is_stop: True\n",
      "  sample          is_stop: False\n",
      "  sentence        is_stop: False\n",
      "  for             is_stop: True\n",
      "  demonstrating   is_stop: False\n",
      "  stop            is_stop: False\n",
      "  word            is_stop: False\n",
      "  removal         is_stop: False\n",
      "  .               is_stop: False\n"
     ]
    }
   ],
   "source": [
    "# spaCy has built-in stop word detection\n",
    "text = \"This is a sample sentence for demonstrating stop word removal.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Token analysis:\")\n",
    "for token in doc:\n",
    "    print(f\"  {token.text:<15} is_stop: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content words: ['sample', 'sentence', 'demonstrating', 'stop', 'word', 'removal']\n"
     ]
    }
   ],
   "source": [
    "# Filter using spaCy's is_stop attribute\n",
    "content_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "print(\"Content words:\", content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spaCy stop words: 326\n"
     ]
    }
   ],
   "source": [
    "# Customize stop words in spaCy\n",
    "# Add custom stop words\n",
    "nlp.vocab[\"sample\"].is_stop = True\n",
    "\n",
    "# Remove words from stop list\n",
    "nlp.vocab[\"not\"].is_stop = False  # 'not' carries meaning!\n",
    "\n",
    "# Check the spaCy stop words list\n",
    "print(f\"Number of spaCy stop words: {len(nlp.Defaults.stop_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Removing Special Characters and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n",
      "Using translate(): Hello World Hows it going NLP user\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Python's string.punctuation\n",
    "print(\"Punctuation characters:\", string.punctuation)\n",
    "\n",
    "# Method 1: Using str.translate()\n",
    "text = \"Hello, World! How's it going? #NLP @user\"\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "clean_text = text.translate(translator)\n",
    "print(\"\\nUsing translate():\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex (letters only): Hello World Hows it going NLP user \n",
      "Regex (alphanumeric): Hello World Hows it going NLP user 123\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using regex\n",
    "import re\n",
    "\n",
    "text = \"Hello, World! How's it going? #NLP @user 123\"\n",
    "\n",
    "# Remove all non-alphanumeric characters (keep spaces)\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "print(\"Regex (letters only):\", clean_text)\n",
    "\n",
    "# Remove punctuation but keep numbers\n",
    "clean_text2 = re.sub(r'[^\\w\\s]', '', text)\n",
    "print(\"Regex (alphanumeric):\", clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean tokens: ['Hello', 'This', 'is', '@user', \"'s\", 'tweet', 'about', 'NLP', 'Check']\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Using spaCy token attributes\n",
    "text = \"Hello! This is @user's tweet about #NLP. Check https://example.com!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filter tokens\n",
    "clean_tokens = [\n",
    "    token.text for token in doc \n",
    "    if not token.is_punct \n",
    "    and not token.is_space\n",
    "    and not token.like_url\n",
    "]\n",
    "\n",
    "print(\"Clean tokens:\", clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text: 'check out new model its amazing'\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 3.2\n",
    "# Clean the following text using regex\n",
    "\n",
    "text = \"Check out @OpenAI's new model! https://openai.com #AI #MachineLearning It's amazing!!!\"\n",
    "\n",
    "clean = text\n",
    "clean = re.sub(r\"http\\S+\", \"\", clean)              # Remove URLs\n",
    "clean = re.sub(r\"@\\w+('s)?\", \"\", clean)            # Remove mentions + optional 's\n",
    "clean = re.sub(r\"#\\w+\", \"\", clean)                 # Remove hashtags\n",
    "clean = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", clean)  # Remove punctuation\n",
    "clean = clean.lower()                              # Lowercase\n",
    "clean = \" \".join(clean.split())                    # Remove extra whitespace\n",
    "\n",
    "print(f\"Clean text: '{clean}'\")\n",
    "assert clean == \"check out new model its amazing\", f\"Got: '{clean}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Complete Preprocessing Pipeline\n",
    "\n",
    "Now let's combine everything into a complete text preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Example Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tokens: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'check', 'information', 'amazing', 'nlp', 'python', 'user']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pipeline_example(text):\n",
    "    \"\"\"\n",
    "    Example preprocessing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Lowercase\n",
    "    2. Remove URLs\n",
    "    3. Remove special characters\n",
    "    4. Tokenize\n",
    "    5. Remove stop words\n",
    "    6. Lemmatize\n",
    "    \"\"\"\n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # Step 3: Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Step 4: Tokenize with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Step 5 & 6: Remove stop words and lemmatize\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if not token.is_stop \n",
    "        and not token.is_punct \n",
    "        and not token.is_space\n",
    "        and len(token.text) > 1  # Remove single characters\n",
    "    ]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test the pipeline\n",
    "sample_text = \"\"\"\n",
    "The quick brown foxes are jumping over the lazy dogs! \n",
    "Check out https://example.com for more information.\n",
    "This is SO amazing!!! #NLP #Python @user123\n",
    "\"\"\"\n",
    "\n",
    "result = preprocess_pipeline_example(sample_text)\n",
    "print(\"Preprocessed tokens:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Final Challenge: Build Your Own Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 4.1 (FINAL CHALLENGE)\n",
    "# Create a complete preprocessing pipeline function that:\n",
    "# 1. Converts text to lowercase\n",
    "# 2. Removes URLs (http/https)\n",
    "# 3. Removes email addresses\n",
    "# 4. Removes mentions (@user) and hashtags (#topic)\n",
    "# 5. Removes numbers\n",
    "# 6. Removes punctuation and special characters\n",
    "# 7. Tokenizes the text\n",
    "# 8. Removes stop words\n",
    "# 9. Applies lemmatization\n",
    "# 10. Removes tokens with less than 2 characters\n",
    "#\n",
    "# The function should return a list of cleaned tokens\n",
    "\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get NLTK English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of preprocessed tokens\n",
    "    \"\"\"\n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove URLs\n",
    "    text = re.sub(r\"http\\S+|https\\S+\", \"\", text)\n",
    "    \n",
    "    # Step 3: Remove emails\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    \n",
    "    # Step 4: Remove mentions (@user) and hashtags (#topic)\n",
    "    text = re.sub(r\"[@#]\\w+('s)?\", \"\", text)\n",
    "    \n",
    "    # Step 5: Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # Step 6: Remove punctuation and special characters\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    \n",
    "    # Step 7: Tokenize using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Step 8 & 9: Remove stop words and lemmatize\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    \n",
    "    # Step 10: Remove tokens with less than 2 characters\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tokens:\n",
      "['break', 'news', 'researcher', 'publish', 'new', 'paper', 'natural', 'language', 'processing', 'check', 'detail', 'contact', 'collaboration', 'experiment', 'conduct', 'stateoftheart', 'transformer', 'model', 'achieve', 'accuracy', 'benchmark', 'dataset']\n"
     ]
    }
   ],
   "source": [
    "# Test your pipeline with this text\n",
    "# Test your pipeline with the provided text\n",
    "test_text = \"\"\"\n",
    " BREAKING NEWS!!! The researchers at @MIT have published 5 new papers \n",
    "about Natural Language Processing! Check out https://mit.edu/nlp for details.\n",
    "Contact them at research@mit.edu for collaborations. #NLP #AI #Research\n",
    "\n",
    "The experiments were conducted using state-of-the-art transformers models.\n",
    "They achieved 95.5% accuracy on the benchmark datasets!!!\n",
    "\"\"\"\n",
    "\n",
    "result = preprocess_text(test_text)\n",
    "print(\"Preprocessed tokens:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Applying Pipeline to Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents:\n",
      "  Doc 1: ['machine', 'learning', 'transform', 'tech', 'industry']\n",
      "  Doc 2: ['love', 'program', 'python', 'easy', 'learn']\n",
      "  Doc 3: ['cat', 'sleep', 'couch', 'lazy']\n",
      "  Doc 4: ['contact', 'question', 'ai', 'product']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Exercise 4.2\n",
    "# Apply your preprocessing pipeline to a list of documents\n",
    "\n",
    "documents = [\n",
    "    \"Machine learning is transforming the tech industry! @Google #ML\",\n",
    "    \"I love programming in Python. It's so easy to learn! https://python.org\",\n",
    "    \"The cats are sleeping on the couch. They're so lazy!\",\n",
    "    \"Contact support@company.com for any questions about our AI products.\"\n",
    "]\n",
    "\n",
    "# Apply the pipeline to each document\n",
    "processed_docs = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "print(\"Processed documents:\")\n",
    "for i, doc in enumerate(processed_docs):\n",
    "    print(f\"  Doc {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Tokenization\n",
    "- **Basic**: `str.split()` - simple but limited\n",
    "- **NLTK**: `word_tokenize()`, `sent_tokenize()` - handles punctuation\n",
    "- **spaCy**: `nlp(text)` - provides rich token information\n",
    "- **Custom**: `RegexpTokenizer` - for specific patterns\n",
    "\n",
    "### Normalization\n",
    "- **Stemming**: Fast, rule-based (Porter, Snowball) - may produce non-words\n",
    "- **Lemmatization**: Accurate, vocabulary-based - produces valid words\n",
    "- **spaCy**: Automatic POS-aware lemmatization with `token.lemma_`\n",
    "\n",
    "### Filtering\n",
    "- **Stop words**: NLTK `stopwords.words()`, spaCy `token.is_stop`\n",
    "- **Punctuation**: `string.punctuation`, spaCy `token.is_punct`\n",
    "- **Special characters**: regex `re.sub()`\n",
    "\n",
    "### Pipeline Best Practices\n",
    "1. Order matters! (lowercase before regex, tokenize before lemmatize)\n",
    "2. Choose stemming vs lemmatization based on your task\n",
    "3. Consider what stop words to keep (e.g., \"not\" for sentiment)\n",
    "4. Test your pipeline on sample data\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "\n",
    "1. Make sure all exercises are completed\n",
    "2. Save this notebook\n",
    "3. Create a Git repository and push your work\n",
    "4. **Send the repository link to: yoroba93@gmail.com**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
